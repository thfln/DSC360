{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a09075-fee7-4063-a7d5-e3cd4c6acc22",
   "metadata": {},
   "source": [
    "# DSC360 - Week 4 - Exercise 4.2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cf23730-cf9b-4c03-b79d-8cd979017123",
   "metadata": {},
   "source": [
    "============================================\n",
    "; Title: Exercise 3.2\n",
    "; Author: Various\n",
    "; Date: 11 September 2024\n",
    "; Modified By: Tyler Heflin\n",
    "; Description: This program demonstrates the use of Python to\n",
    "; perform Natural Language Processing (NLP).\n",
    ";=========================================== */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947342f-20a8-4721-a0a6-efd8a8e059ca",
   "metadata": {},
   "source": [
    "We begin the exercise this week by importing the necessary libraries and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a379a389-405a-4dd1-8479-8dd5c3357de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from autocorrect import Speller\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60994f-171b-4131-bd2c-dcdfcaf56a2f",
   "metadata": {},
   "source": [
    "## Using Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb056de-a559-4007-98b6-2b8566282ef2",
   "metadata": {},
   "source": [
    "**1. In the text, there's a text normalizer created - your assignment is to re-create that normalizer as a Python class that can be re-used (within a .py file). However, unlike the book author's version, pass a Pandas Series (e.g., dataframe['column']) to your normalize_corpus function and use apply/lambda for each cleaning function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5b5cf3-2b24-4705-839d-3f9a6511c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text normalizer as a class\n",
    "class TextNormalizer:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell = Speller()\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Define function to obtain part of speech tags\n",
    "    def get_wordnet_pos(self, word):\n",
    "        tag = pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    # Define function to clean specified text\n",
    "    def clean_text(self, text):\n",
    "        # Remove HTML tags\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        # Expand contractions\n",
    "        text = contractions.fix(text)\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Make lowercase and remove punctuation/stopwords\n",
    "        tokens = [self.spell(word.lower()) for word in tokens if word.isalpha() and word.lower() not in self.stop_words]\n",
    "        # Lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(word, self.get_wordnet_pos(word)) for word in tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Define normalization function (including apply/lambda function)\n",
    "    def normalize(self, text_series):\n",
    "        return text_series.apply(lambda text:self.clean_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd108b-79e0-4e78-a83b-57b3048ba38c",
   "metadata": {},
   "source": [
    "**2. Using your new text normalizer, create a Jupyter Notebook that uses this class to clean up the text found in the file \"big.txt\" (that text file is in the GitHub for Week 4 repository). Your resulting text should be a (long) single stream of text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4b6283-5144-40ca-afef-41531c3c9a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    project gutenberg ebook adventure sherlock hol...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "file_path = r'C:\\Users\\Amanda Heflin\\Downloads\\big.txt'\n",
    "\n",
    "# Read file\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Convert into pandas series and return sample of cleaned text\n",
    "text_series = pd.Series([text[:1000]])\n",
    "normalizer = TextNormalizer()\n",
    "cleaned_text = normalizer.normalize(text_series)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd2f80-91f2-4193-93d6-12bd4cc6d165",
   "metadata": {},
   "source": [
    "**3. Then, using spaCy *and* NLTK (this will be two different ways of doing the same thing), create code that show the tokens, lemmas, parts of speech, and dependencies in the first 1,000 characters of \"big.txt\" (the same text you normalized).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0eb093c-ce22-4a46-bf15-e6d4edebfd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Results:\n",
      "('project', 'project', 'PROPN', 'compound')\n",
      "('gutenberg', 'gutenberg', 'PROPN', 'compound')\n",
      "('ebook', 'ebook', 'PROPN', 'compound')\n",
      "('adventure', 'adventure', 'NOUN', 'compound')\n",
      "('sherlock', 'sherlock', 'PROPN', 'compound')\n",
      "('holmes', 'holmes', 'PROPN', 'compound')\n",
      "('sir', 'sir', 'PROPN', 'compound')\n",
      "('arthur', 'arthur', 'PROPN', 'compound')\n",
      "('conan', 'conan', 'PROPN', 'compound')\n",
      "('doyle', 'doyle', 'PROPN', 'compound')\n",
      "('series', 'series', 'PROPN', 'nsubj')\n",
      "('sir', 'sir', 'PROPN', 'compound')\n",
      "('arthur', 'arthur', 'PROPN', 'compound')\n",
      "('conan', 'conan', 'PROPN', 'compound')\n",
      "('doyle', 'doyle', 'PROPN', 'compound')\n",
      "('copyright', 'copyright', 'NOUN', 'compound')\n",
      "('law', 'law', 'NOUN', 'compound')\n",
      "('change', 'change', 'NOUN', 'relcl')\n",
      "('world', 'world', 'NOUN', 'dobj')\n",
      "('sure', 'sure', 'ADV', 'advmod')\n",
      "('check', 'check', 'VERB', 'ROOT')\n",
      "('copyright', 'copyright', 'NOUN', 'compound')\n",
      "('law', 'law', 'NOUN', 'compound')\n",
      "('country', 'country', 'PROPN', 'compound')\n",
      "('download', 'download', 'NOUN', 'compound')\n",
      "('distribute', 'distribute', 'VERB', 'compound')\n",
      "('project', 'project', 'PROPN', 'compound')\n",
      "('gutenberg', 'gutenberg', 'PROPN', 'compound')\n",
      "('ebook', 'ebook', 'NOUN', 'compound')\n",
      "('header', 'header', 'NOUN', 'nmod')\n",
      "('first', 'first', 'ADJ', 'amod')\n",
      "('thing', 'thing', 'NOUN', 'nsubj')\n",
      "('see', 'see', 'VERB', 'ccomp')\n",
      "('view', 'view', 'NOUN', 'compound')\n",
      "('project', 'project', 'NOUN', 'compound')\n",
      "('gutenberg', 'gutenberg', 'PROPN', 'compound')\n",
      "('file', 'file', 'NOUN', 'nsubj')\n",
      "('please', 'please', 'INTJ', 'intj')\n",
      "('remove', 'remove', 'VERB', 'ccomp')\n",
      "('change', 'change', 'NOUN', 'compound')\n",
      "('edit', 'edit', 'NOUN', 'compound')\n",
      "('header', 'header', 'NOUN', 'dobj')\n",
      "('without', 'without', 'ADP', 'prep')\n",
      "('write', 'write', 'ADJ', 'amod')\n",
      "('permission', 'permission', 'NOUN', 'pobj')\n",
      "('please', 'please', 'INTJ', 'intj')\n",
      "('read', 'read', 'VERB', 'conj')\n",
      "('legal', 'legal', 'ADJ', 'amod')\n",
      "('small', 'small', 'ADJ', 'amod')\n",
      "('print', 'print', 'NOUN', 'compound')\n",
      "('information', 'information', 'NOUN', 'compound')\n",
      "('ebook', 'ebook', 'NOUN', 'compound')\n",
      "('project', 'project', 'NOUN', 'compound')\n",
      "('gutenberg', 'gutenberg', 'PROPN', 'compound')\n",
      "('bottom', 'bottom', 'PROPN', 'compound')\n",
      "('file', 'file', 'NOUN', 'dobj')\n",
      "('include', 'include', 'VERB', 'conj')\n",
      "('important', 'important', 'ADJ', 'amod')\n",
      "('information', 'information', 'NOUN', 'npadvmod')\n",
      "('specific', 'specific', 'ADJ', 'amod')\n",
      "('right', 'right', 'ADJ', 'amod')\n",
      "('restriction', 'restriction', 'NOUN', 'compound')\n",
      "('file', 'file', 'NOUN', 'nsubj')\n",
      "('may', 'may', 'AUX', 'aux')\n",
      "('use', 'use', 'AUX', 'aux')\n",
      "('also', 'also', 'ADV', 'advmod')\n",
      "('find', 'find', 'VERB', 'ccomp')\n",
      "('make', 'make', 'VERB', 'xcomp')\n",
      "('donation', 'donation', 'NOUN', 'compound')\n",
      "('project', 'project', 'NOUN', 'compound')\n",
      "('gutenberg', 'gutenberg', 'PROPN', 'nsubj')\n",
      "('get', 'get', 'AUX', 'aux')\n",
      "('involve', 'involve', 'VERB', 'ccomp')\n",
      "('welcome', 'welcome', 'ADJ', 'amod')\n",
      "('world', 'world', 'PROPN', 'npadvmod')\n",
      "('free', 'free', 'ADJ', 'amod')\n",
      "('plain', 'plain', 'ADJ', 'amod')\n",
      "('vanilla', 'vanilla', 'NOUN', 'compound')\n",
      "('electronic', 'electronic', 'ADJ', 'amod')\n",
      "('text', 'text', 'NOUN', 'compound')\n",
      "('ebooks', 'ebook', 'NOUN', 'advcl')\n",
      "('readable', 'readable', 'ADJ', 'amod')\n",
      "('human', 'human', 'ADJ', 'amod')\n",
      "('computer', 'computer', 'NOUN', 'dobj')\n",
      "('since', 'since', 'SCONJ', 'mark')\n",
      "('ebooks', 'ebook', 'NOUN', 'nsubj')\n",
      "('prepared', 'prepare', 'VERB', 'advcl')\n",
      "('thousand', 'thousand', 'NUM', 'npadvmod')\n",
      "\n",
      "NLTK Results:\n",
      "Token: project, Lemma: project, POS: NN\n",
      "Token: gutenberg, Lemma: gutenberg, POS: NN\n",
      "Token: ebook, Lemma: ebook, POS: NN\n",
      "Token: adventure, Lemma: adventure, POS: NN\n",
      "Token: sherlock, Lemma: sherlock, POS: NN\n",
      "Token: holmes, Lemma: holmes, POS: NNS\n",
      "Token: sir, Lemma: sir, POS: VBP\n",
      "Token: arthur, Lemma: arthur, POS: RB\n",
      "Token: conan, Lemma: conan, POS: JJ\n",
      "Token: doyle, Lemma: doyle, POS: JJ\n",
      "Token: series, Lemma: series, POS: NN\n",
      "Token: sir, Lemma: sir, POS: NN\n",
      "Token: arthur, Lemma: arthur, POS: IN\n",
      "Token: conan, Lemma: conan, POS: JJ\n",
      "Token: doyle, Lemma: doyle, POS: NN\n",
      "Token: copyright, Lemma: copyright, POS: NN\n",
      "Token: law, Lemma: law, POS: NN\n",
      "Token: change, Lemma: change, POS: NN\n",
      "Token: world, Lemma: world, POS: NN\n",
      "Token: sure, Lemma: sure, POS: JJ\n",
      "Token: check, Lemma: check, POS: VB\n",
      "Token: copyright, Lemma: copyright, POS: JJ\n",
      "Token: law, Lemma: law, POS: NN\n",
      "Token: country, Lemma: country, POS: NN\n",
      "Token: download, Lemma: download, POS: VBZ\n",
      "Token: distribute, Lemma: distribute, POS: JJ\n",
      "Token: project, Lemma: project, POS: NN\n",
      "Token: gutenberg, Lemma: gutenberg, POS: NN\n",
      "Token: ebook, Lemma: ebook, POS: NN\n",
      "Token: header, Lemma: header, POS: NN\n",
      "Token: first, Lemma: first, POS: JJ\n",
      "Token: thing, Lemma: thing, POS: NN\n",
      "Token: see, Lemma: see, POS: NN\n",
      "Token: view, Lemma: view, POS: NN\n",
      "Token: project, Lemma: project, POS: NN\n",
      "Token: gutenberg, Lemma: gutenberg, POS: NN\n",
      "Token: file, Lemma: file, POS: NN\n",
      "Token: please, Lemma: please, POS: NN\n",
      "Token: remove, Lemma: remove, POS: VB\n",
      "Token: change, Lemma: change, POS: NN\n",
      "Token: edit, Lemma: edit, POS: NN\n",
      "Token: header, Lemma: header, POS: NN\n",
      "Token: without, Lemma: without, POS: IN\n",
      "Token: write, Lemma: write, POS: JJ\n",
      "Token: permission, Lemma: permission, POS: NN\n",
      "Token: please, Lemma: please, POS: NN\n",
      "Token: read, Lemma: read, POS: VB\n",
      "Token: legal, Lemma: legal, POS: JJ\n",
      "Token: small, Lemma: small, POS: JJ\n",
      "Token: print, Lemma: print, POS: NN\n",
      "Token: information, Lemma: information, POS: NN\n",
      "Token: ebook, Lemma: ebook, POS: NN\n",
      "Token: project, Lemma: project, POS: NN\n",
      "Token: gutenberg, Lemma: gutenberg, POS: JJ\n",
      "Token: bottom, Lemma: bottom, POS: NN\n",
      "Token: file, Lemma: file, POS: NN\n",
      "Token: include, Lemma: include, POS: VBP\n",
      "Token: important, Lemma: important, POS: JJ\n",
      "Token: information, Lemma: information, POS: NN\n",
      "Token: specific, Lemma: specific, POS: JJ\n",
      "Token: right, Lemma: right, POS: JJ\n",
      "Token: restriction, Lemma: restriction, POS: NN\n",
      "Token: file, Lemma: file, POS: NN\n",
      "Token: may, Lemma: may, POS: MD\n",
      "Token: use, Lemma: use, POS: VB\n",
      "Token: also, Lemma: also, POS: RB\n",
      "Token: find, Lemma: find, POS: VBP\n",
      "Token: make, Lemma: make, POS: JJ\n",
      "Token: donation, Lemma: donation, POS: NN\n",
      "Token: project, Lemma: project, POS: NN\n",
      "Token: gutenberg, Lemma: gutenberg, POS: NN\n",
      "Token: get, Lemma: get, POS: VB\n",
      "Token: involve, Lemma: involve, POS: JJ\n",
      "Token: welcome, Lemma: welcome, POS: JJ\n",
      "Token: world, Lemma: world, POS: NN\n",
      "Token: free, Lemma: free, POS: JJ\n",
      "Token: plain, Lemma: plain, POS: NN\n",
      "Token: vanilla, Lemma: vanilla, POS: NN\n",
      "Token: electronic, Lemma: electronic, POS: JJ\n",
      "Token: text, Lemma: text, POS: NN\n",
      "Token: ebooks, Lemma: ebooks, POS: VBZ\n",
      "Token: readable, Lemma: readable, POS: JJ\n",
      "Token: human, Lemma: human, POS: JJ\n",
      "Token: computer, Lemma: computer, POS: NN\n",
      "Token: since, Lemma: since, POS: IN\n",
      "Token: ebooks, Lemma: ebooks, POS: NNS\n",
      "Token: prepared, Lemma: prepared, POS: JJ\n",
      "Token: thousand, Lemma: thousand, POS: VBP\n",
      "\n",
      "NLTK Dependency Approximation:\n",
      "NP:[('project', 'NN')]\n",
      "NP:[('gutenberg', 'NN')]\n",
      "NP:[('ebook', 'NN')]\n",
      "NP:[('adventure', 'NN')]\n",
      "NP:[('sherlock', 'NN')]\n",
      "NP:[('holmes', 'NNS')]\n",
      "VP:[('sir', 'VBP'), ('arthur', 'RB'), ('conan', 'JJ'), ('doyle', 'JJ'), ('series', 'NN'), ('sir', 'NN')]\n",
      "PP:[('arthur', 'IN'), ('conan', 'JJ'), ('doyle', 'NN')]\n",
      "NP:[('copyright', 'NN')]\n",
      "NP:[('law', 'NN')]\n",
      "NP:[('change', 'NN')]\n",
      "NP:[('world', 'NN')]\n",
      "NP:[('copyright', 'JJ'), ('law', 'NN')]\n",
      "NP:[('country', 'NN')]\n",
      "NP:[('distribute', 'JJ'), ('project', 'NN')]\n",
      "NP:[('gutenberg', 'NN')]\n",
      "NP:[('ebook', 'NN')]\n",
      "NP:[('header', 'NN')]\n",
      "NP:[('first', 'JJ'), ('thing', 'NN')]\n",
      "NP:[('see', 'NN')]\n",
      "NP:[('view', 'NN')]\n",
      "NP:[('project', 'NN')]\n",
      "NP:[('gutenberg', 'NN')]\n",
      "NP:[('file', 'NN')]\n",
      "NP:[('please', 'NN')]\n",
      "NP:[('change', 'NN')]\n",
      "NP:[('edit', 'NN')]\n",
      "NP:[('header', 'NN')]\n",
      "PP:[('without', 'IN'), ('write', 'JJ'), ('permission', 'NN')]\n",
      "NP:[('please', 'NN')]\n",
      "NP:[('legal', 'JJ'), ('small', 'JJ'), ('print', 'NN')]\n",
      "NP:[('information', 'NN')]\n",
      "NP:[('ebook', 'NN')]\n",
      "NP:[('project', 'NN')]\n",
      "NP:[('gutenberg', 'JJ'), ('bottom', 'NN')]\n",
      "NP:[('file', 'NN')]\n",
      "NP:[('important', 'JJ'), ('information', 'NN')]\n",
      "NP:[('specific', 'JJ'), ('right', 'JJ'), ('restriction', 'NN')]\n",
      "NP:[('file', 'NN')]\n",
      "VP:[('use', 'VB'), ('also', 'RB')]\n",
      "NP:[('make', 'JJ'), ('donation', 'NN')]\n",
      "NP:[('project', 'NN')]\n",
      "NP:[('gutenberg', 'NN')]\n",
      "NP:[('involve', 'JJ'), ('welcome', 'JJ'), ('world', 'NN')]\n",
      "NP:[('free', 'JJ'), ('plain', 'NN')]\n",
      "NP:[('vanilla', 'NN')]\n",
      "NP:[('electronic', 'JJ'), ('text', 'NN')]\n",
      "NP:[('readable', 'JJ'), ('human', 'JJ'), ('computer', 'NN')]\n",
      "PP:[('since', 'IN'), ('ebooks', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, RegexpParser, pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to utilize treebank tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Use spaCy for analysis\n",
    "doc_spacy = normalizer.nlp(cleaned_text.iloc[0])\n",
    "spacy_results = [(token.text, token.lemma_, token.pos_, token.dep_) for token in doc_spacy]\n",
    "\n",
    "# Display results\n",
    "print(\"spaCy Results:\")\n",
    "for result in spacy_results:\n",
    "    print(result)\n",
    "\n",
    "# Tokenization and Part of Speech tagging with NLTK\n",
    "tokens_nltk = word_tokenize(cleaned_text.iloc[0])\n",
    "pos_tags = pos_tag(tokens_nltk)\n",
    "# Lemmatize tokens based on POS tags\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "# Define expanded grammar model that includes adj and adv\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ.*>*<NN.*>} # Noun Phrase with adj\n",
    "VP: {<VB.*><RB.*><NP|PP>*} # Verb Phrase with adv\n",
    "PP: {<IN><NP>} # Prepositional Phrase\n",
    "\"\"\"\n",
    "parser = RegexpParser(grammar)\n",
    "chunked = parser.parse(pos_tags)\n",
    "\n",
    "# Display results including chunked structure for dependency approximation\n",
    "print(\"\\nNLTK Results:\")\n",
    "for i, (word, tag) in enumerate(pos_tags):\n",
    "    lemma = lemmas[i]\n",
    "    print(f\"Token: {word}, Lemma: {lemma}, POS: {tag}\")\n",
    "\n",
    "print(\"\\nNLTK Dependency Approximation:\")\n",
    "for subtree in chunked:\n",
    "    if isinstance(subtree, nltk.Tree):\n",
    "        print(f\"{subtree.label()}:{subtree.leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66a034-8d32-4155-bdd8-f48a6da97bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
